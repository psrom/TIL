# 벡터가 어떻게 의미를 가지게 되는가

## 2.1 자연어 계산과 이해

임베딩을 만드는 세 가지 철학

| 구분        | bag  of words 가정                               | 언어 모델                       | 분포 가정                     |
| ----------- | ------------------------------------------------ | ------------------------------- | ----------------------------- |
| 내용        | 어떤 단어가 __많이__ 쓰였는가                    | 단어가 어떤 __순서__로 쓰였는가 | 어떤 단어가 __같이__ 쓰였는가 |
| 대표 통계량 | TF-IDF, Counter, Hashing                         | -                               | PMI(점별 상호 정보량)         |
| 대표 모델   | Deep Averaging Network<br />(단어의 순서 고려 X) | ELMo, GPT                       | Word2Vec                      |



## 2.2 어떤 단어가 많이 쓰였는가

### 2.2.1 bag of words 가정

- __bag__ : 중복 원소를 허용한 __집합__
- 정보 검색(Information Retrieval) 분야에서 많이 사용 된다.



### 2.2.2 TF-IDF

- Term Frequency-Inverse Document Frequency
- bag of words의 단점을 보완하기 위해(조사 빈도 엄청 높은 것과 같은 단점)
- __TF__ : 어떤 단어가 특정 문서에 얼마나 많이 쓰였는가
- __DF__ : 특정 단어가 나타난 문서의 수
- __IDF__ : `log(DF/N(전체 문서 수))`



### 2.2.3 Deep Averaging Network(DAN)

- 단어의 순서 고려 X