> 한국어 임베딩, 이기창 지음

# 1. 임베딩이란

- 정의: 사람이 쓰는 자연어를 기계가 이해할 수 있는 숫자의 나열인 **벡터**로 바꾼 결과



## [1] 임베딩의 역할

- 단어/문장 간 관련도 계산(유사도)
- 의미적/문법적 정보 함축(latent)
- 전이 학습

---

### 1. 단어/문장 간 관련도 계산

- 단어-문서 행렬
- __Word2Vec__ - 코사인 유사도 이용
- 시각화: __t-SNE__ (차원 축소 기법)



### 2. 의미/문법 정보 함축

- 임베딩 사칙 연산 가능
- __단어 유추 평가__ : 단어 임베딩을 평가하는 방법



### 3. 전이 학습

- 전이학습: 임베딩을 __다른 딥러닝 모델의 입력값__으로 사용



## [2] 임베딩 기법

- 통계 기반
- 뉴럴 네트워크

---

1. __통계 기반: 잠재 의미 분석(LSA | Latent Semantic Analysis)__

- 정의: 단어 사용 빈도 등 말뭉치의 통계량 정보가 들어 있는 행렬에 __특이값 분해(Singular Value Decomposition)__ 등 수학적 기법을 작용해 행렬에 속한 벡터들의 차원을 축소하는 방법

- 대상 행렬

  - TF-IDF 행렬

  - Word-Context Matrix

  - 점별 상호 정보량 행렬(Pointwise Mutual Information Matrix)



2. __뉴럴 네트워크__

- 이전 단어가 주어졌을 때 다음 단어가 무엇일지 예측
- masking 한 단어가 무엇인지 예측

---

### 단어 수준에서 문장 수준으로

- 단어 수준 임베딩 기법의 단점: __동음이의어__
- 문장 수준 임베딩
  - ELMo(Embeddings from Language Models)
  - BERT(Bidirectional Encoder Retpresentations from Transformer)
  - GPT(Generative Pre-Training)

---

### 룰 :arrow_right: 엔드투엔드 :arrow_right: 프리트레인/파인튜닝

1990년대: 사람이 feature를 직접 뽑음

2000년대 중반 이후: __딥러닝 모델__

- 입출력 사이의 관계를 잘 근사(approximation)
- 모델에게 규칙을 직접 알려주지 않아도 괜찮다.
- 사람의 개입 없이 모델 스스로 학습 __end-to-end model__
  - 기계번역: __sequence-to-sequence model__

2018년: __ELMo Model__

- __pretrain & fine tuning__
- pretrain: 대규모 말뭉치로 임베딩 생성
- fine tuning, 전이학습: 임베딩을 입력으로 하는 새로운 딥러닝 모델을 만들고 풀고 싶은 문제에 맞게 모델 전체를 업데이트
- ELMo, GPT, BERT(모두 문장 수준 임베딩)

___

### 임베딩 종류

- 행렬 분해 기반 방법(factorization)
  - sum,  concatenate
  - 예) Glove, Swivel
- 예측 기반 방법
  - 예) Word2Vec, FastText, BERT, ELMo, GPT
- 토픽 기반 방법
  - 주어진 문서에 잠재된 주제를 추론하는 방식
  - 자매 디리클레 할당(Latent Dirichlet Allocation)
  - 예) LDA
