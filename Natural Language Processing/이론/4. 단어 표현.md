# 단어표현

## 1. 카운트 기반 방법

카운트 기반으로 어떤 글의 문맥 안에 단어가 동시에 등장한 횟수를 세는 방법 :arrow_right: **동시 출현 행렬(Co-occurrence)**

- 특이값 분해(Singular Value Decomposition, SVD)
- 잠재의미분석(Latent Semantic Analysis, LSA)
- Hyperspace Analogue to Language(HAL)
- Helinger PCA (Principal Component Analysis)

**장점**: 적은 시간으로 단어 벡터 생성 가능

---

### 코드

```python
from sklearn.feature_extraction.text import CountVectorizer

text = ['성진과 창욱은 야구장에 갔다',
       '성진과 태균은 도서관에 갔다',
       '성진과 창욱은 공부를 좋아한다']

cv = CountVectorizer()
x = cv.fit_transform(text).toarray()
x
```

```python
# output
array([[1, 0, 0, 1, 1, 0, 1, 0],
       [1, 0, 1, 1, 0, 0, 0, 1],
       [0, 1, 0, 1, 0, 1, 1, 0]])
```

```python
import numpy as np
C = np.dot(x.T, x)
np.fill_diagonal(C, 0)
print(C)
```

```python
# output
[[0 0 1 2 1 0 1 1]
 [0 0 0 1 0 1 1 0]
 [1 0 0 1 0 0 0 1]
 [2 1 1 0 1 1 2 1]
 [1 0 0 1 0 0 1 0]
 [0 1 0 1 0 0 1 0]
 [1 1 0 2 1 1 0 0]
 [1 0 1 1 0 0 0 0]]
```

```python
word2idx = cv.vocabulary_
idx2word = {v:k for (k, v) in word2idx.items()}
word2idx
```

```python
# output
{'갔다': 0,
 '공부를': 1,
 '도서관에': 2,
 '성진과': 3,
 '야구장에': 4,
 '좋아한다': 5,
 '창욱은': 6,
 '태균은': 7}
```

---

## 2. 예측 방법(학습 기반)

신경망 구조 혹은 어떤 모델을 사용하여 특정 문맥에서 어떤 단어가 나올지를 예측하면서 단어를 벡터로 만드는 방식

- word2vec
  - CBOW(Continuous Bag of Words)
  - Skip-Gram
- NNLM(Neural Network Language Model)
- RNNLM(Recurrent Neural Network Language Model)

---



![word2vec](4.%20%EB%8B%A8%EC%96%B4%20%ED%91%9C%ED%98%84/scode=mtistory2&fname=https%253A%252F%252Fblog.kakaocdn.net%252Fdn%252F2Qj2I%252Fbtrb7zSiErG%252FfxZChADnf1iQ7zCx43W5o0%252Fimg.png)

> https://wooono.tistory.com/244

### word2vec

#### [1] CBOW

1. 각 주변 단어를 one-hot vector로 만들어 입력값으로 사용(입력층)
2. 가중치 행렬(weight matrix)을 각 one-hot vector에 곱해서 n-차원 벡터 생성(n차원 은닉층)
3. n-차원 벡터를 모두 더한 후 개수로 나눠 평균 n-차원 벡터 생성(출력층)
4. n-차원 벡터에 다시 가중치 행렬을 곱해서 one-hot vector와 같은 차원의 vector 생성
5. 만들어진 vector를 실제 예측하려는 단어의 one-hot vector와 비교하여 학습

#### [2] Skip-Gram

1. 하나의 단어를 one-hot vector로 만들어서 입력값으로 사용(입력층)
2. 가중치 행렬을 one-hot vector에 곱해서 n-차원 벡터 생성(은닉층)
3. n-차원 벡터에 다시 가중치 행렬을 곱해서 one-hot vector와 같은 차원의 벡터 생성(출력층)
4. 만들어진 vector를 실제 예측하려는 단어의 one-hot vector와 비교하여 학습