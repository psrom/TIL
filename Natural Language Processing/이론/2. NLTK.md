[toc]

**자연어 토크나이징 도구**

- 자연어 처리를 위해선 텍스트에 대한 정보를 단위별로 나누는 게 우선이다.
- 이때, `token`으로 나눈다고 한다.

# NLTK

- 영어 토크나이징 라이브러리 중 하나
- Natural Language Toolkit

- **토크나이징**: 텍스트에 대해 특정 기준 단위로 문장을 나누는 것
  1. 단어 단위
  2. 문장 단위



---

```python
import nltk
nltk.download('punkt')
nltk.download('stopwords') #불용어(is, a, an, the, …)
```

## 1-1. 단어 단위 tokenize

```python
from nltk.tokenize import word_tokenize
sentence = """
Natural language processing (NLP) is a subfield of computer science, information engineering, 
and artificial intelligence concerned with the interactions between computers and human (natural) languages, 
in particular how to program computers to process and analyze large amounts of natural language data.
"""

print(word_tokenize(sentence))
```

```python
# output
['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'computer', 'science', ',', 'information', 'engineering', ',', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', ',', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.']
```

## 1-2. 문장 단위 tokenize

```python
from nltk.tokenize import sent_tokenize
paragraph = """
Natural language processing (NLP) is a subfield of computer science, information engineering, 
and artificial intelligence concerned with the interactions between computers and human (natural) languages, 
in particular how to program computers to process and analyze large amounts of natural language data.
"""

print(sent_tokenize(paragraph))
```

```python
# output
['\nNatural language processing (NLP) is a subfield of computer science, information engineering, \nand artificial intelligence concerned with the interactions between computers and human (natural) languages, \nin particular how to program computers to process and analyze large amounts of natural language data.', 'Challenges in natural language processing frequently involve speech recognition, natural language understanding, \nand natural language generation.']
```

___

## 2. 텍스트 단어 분리

```python
word_tokens = [word_tokenize(x) for x in sent_tokenize(paragraph)]
print(word_tokens[1])
```

```python
# output
['Challenges',
 'in',
 'natural',
 'language',
 'processing',
 'frequently',
 'involve',
 'speech',
 'recognition',
 ',',
 'natural',
 'language',
 'understanding',
 ',',
 'and',
 'natural',
 'language',
 'generation',
 '.']
```

```python
>>> word_tokens[1][0]
'Challenges'
```

___

## 3. stop words 제거

```python
stopwords = nltk.corpus.stopwords.words('english')
stopwords.append(',')
stopwords.append(')')
stopwords.append('(')
```

```python
all_tokens = []
for sent in word_tokens:
    all_tokens.append([word for word in sent if word.lower() not in stopwords]) # 소문자 변환 후 불용어 제외
print(all_tokens)
```

<details>
    <summary>output</summary>
    <div markdown='1'>
        [['Natural',
  'language',
  'processing',
  'NLP',
  'subfield',
  'computer',
  'science',
  'information',
  'engineering',
  'artificial',
  'intelligence',
  'concerned',
  'interactions',
  'computers',
  'human',
  'natural',
  'languages',
  'particular',
  'program',
  'computers',
  'process',
  'analyze',
  'large',
  'amounts',
  'natural',
  'language',
  'data',
  '.'],
 ['Challenges',
  'natural',
  'language',
  'processing',
  'frequently',
  'involve',
  'speech',
  'recognition',
  'natural',
  'language',
  'understanding',
  'natural',
  'language',
  'generation',
  '.']]
        </div>
    </details>

---

## 4. Stemming

어간을 찾는 기능

```python
from nltk.stem import LancasterStemmer
stemmer = LancasterStemmer()

for word in ['working', 'works', 'worked']:
    print(stemmer.stem(word))
```

```python
# output
work
work
work
```

___

```python
# all_tokens의 불용어 제거 후 stemming
tokens_stem = []
for sent in all_tokens:
    sentence = []
    for word in sent:
        sentence.append(stemmer.stem(word))
    tokens_stem.append(sentence)
print(tokens_stem)
```

<details>
    <summary>output</summary>
    <div markdown='1'>
        [['nat',
  'langu',
  'process',
  'nlp',
  'subfield',
  'comput',
  'sci',
  'inform',
  'engin',
  'art',
  'intellig',
  'concern',
  'interact',
  'comput',
  'hum',
  'nat',
  'langu',
  'particul',
  'program',
  'comput',
  'process',
  'analys',
  'larg',
  'amount',
  'nat',
  'langu',
  'dat',
  '.'],
 ['challeng',
  'nat',
  'langu',
  'process',
  'frequ',
  'involv',
  'speech',
  'recognit',
  'nat',
  'langu',
  'understand',
  'nat',
  'langu',
  'gen',
  '.']]
    </div>
</details>

___

## 5. Lemmatization

```python
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet') # 네트워크 형태의 사전

lemma = WordNetLemmatizer()
for word in ['working', 'words', 'worked']:
    print(lemma.lemmatize(word, 'v')) # 동사

for word in ['happier', 'happiest']:
    print(lemma.lemmatize(word, 'a')) # 형용사
```

```python
# output
work
work
happy
happy
```

---

## 6. 사전(vocabulary) 생성

```python
word2idx = {}
n_idx = 0
for sent in all_tokens:
    for word in sent:
        if word.lower() not in word2idx:
            word2idx[word.lower()] = n_idx
            n_idx += 1
idx2word = {v:k for k, v in word2idx.items()}
```

```python
print(word2idx)
```

<details>
    <summary>output</summary>
    <div markdown='1'>
       {'.': 23,
 'amounts': 21,
 'analyze': 19,
 'artificial': 9,
 'challenges': 24,
 'computer': 5,
 'computers': 13,
 'concerned': 11,
 'data': 22,
 'engineering': 8,
 'frequently': 25,
 'generation': 30,
 'human': 14,
 'information': 7,
 'intelligence': 10,
 'interactions': 12,
 'involve': 26,
 'language': 1,
 'languages': 15,
 'large': 20,
 'natural': 0,
 'nlp': 3,
 'particular': 16,
 'process': 18,
 'processing': 2,
 'program': 17,
 'recognition': 28,
 'science': 6,
 'speech': 27,
 'subfield': 4,
 'understanding': 29} 
    </div>
</details>

```python
print(idx2word)
```

<details>
    <summary>output</summary>
    <div markdown='1'>
        {0: 'natural',
 1: 'language',
 2: 'processing',
 3: 'nlp',
 4: 'subfield',
 5: 'computer',
 6: 'science',
 7: 'information',
 8: 'engineering',
 9: 'artificial',
 10: 'intelligence',
 11: 'concerned',
 12: 'interactions',
 13: 'computers',
 14: 'human',
 15: 'languages',
 16: 'particular',
 17: 'program',
 18: 'process',
 19: 'analyze',
 20: 'large',
 21: 'amounts',
 22: 'data',
 23: '.',
 24: 'challenges',
 25: 'frequently',
 26: 'involve',
 27: 'speech',
 28: 'recognition',
 29: 'understanding',
 30: 'generation'}
    </div>
</details>

```python
# text를 사전의 index로 표현
text_idx = []
for sent in all_tokens:
    sent_idx = []
    for word in sent:
        sent_idx.append(word2idx[word.lower()])
    text_idx.append(sent_idx)
print(text_idx[0])
```

<details>
    <summary>output</summary>
    <div markdown='1'>
        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 0, 15, 16, 17, 13, 18, 19, 20, 21, 0, 1, 22, 23]
    </div>
</details>

```python
# text_idx를 다시 단어로 표현
text = []
for sent_idx in text_idx:
    sent = []
    for word_idx in sent_idx:
        sent.append(idx2word[word_idx])
    text.append(sent)
print(text[0])
```

<details>
    <summary>output</summary>
    <div markdown='1'>
        ['natural', 'language', 'processing', 'nlp', 'subfield', 'computer', 'science', 'information', 'engineering', 'artificial', 'intelligence', 'concerned', 'interactions', 'computers', 'human', 'natural', 'languages', 'particular', 'program', 'computers', 'process', 'analyze', 'large', 'amounts', 'natural', 'language', 'data', '.']
    </div>
</details>

---

# NLTK(pos)

**Pos**: Part of Speech tagging

```python
import nltk
nltk.download('punkt') #'/root/nltk_data/tokenizers'
nltk.download('averaged_perceptron_tagger') # 품사 tagging 분석을 위해 필요한 도구
```

```python
sentence = """
Natural language processing (NLP) is a subfield of computer science, information engineering, 
and artificial intelligence concerned with the interactions between computers and human (natural) languages, 
in particular how to program computers to process and analyze large amounts of natural language data.
"""
word_tok = nltk.word_tokenize(sentence)
print(word_tok)
```

<details>
    <summary>output</summary>
    <div markdown='1'>
        ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'computer', 'science', ',', 'information', 'engineering', ',', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', ',', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.']
    </div>
</details>

---

## 1. nltk.pos_tag()

```python
>>> print(nltk.pos_tag(word_tok))
[('Natural', 'JJ'),
 ('language', 'NN'),
 ('processing', 'NN'),
...
 ('language', 'NN'),
 ('data', 'NNS'),
 ('.', '.')]
```

<details>
    <summary>output</summary>
    <div markdown='1'>
        [('Natural', 'JJ'),
 ('language', 'NN'),
 ('processing', 'NN'),
 ('(', '('),
 ('NLP', 'NNP'),
 (')', ')'),
 ('is', 'VBZ'),
 ('a', 'DT'),
 ('subfield', 'NN'),
 ('of', 'IN'),
 ('computer', 'NN'),
 ('science', 'NN'),
 (',', ','),
 ('information', 'NN'),
 ('engineering', 'NN'),
 (',', ','),
 ('and', 'CC'),
 ('artificial', 'JJ'),
 ('intelligence', 'NN'),
 ('concerned', 'VBN'),
 ('with', 'IN'),
 ('the', 'DT'),
 ('interactions', 'NNS'),
 ('between', 'IN'),
 ('computers', 'NNS'),
 ('and', 'CC'),
 ('human', 'JJ'),
 ('(', '('),
 ('natural', 'JJ'),
 (')', ')'),
 ('languages', 'NNS'),
 (',', ','),
 ('in', 'IN'),
 ('particular', 'JJ'),
 ('how', 'WRB'),
 ('to', 'TO'),
 ('program', 'NN'),
 ('computers', 'NNS'),
 ('to', 'TO'),
 ('process', 'VB'),
 ('and', 'CC'),
 ('analyze', 'VB'),
 ('large', 'JJ'),
 ('amounts', 'NNS'),
 ('of', 'IN'),
 ('natural', 'JJ'),
 ('language', 'NN'),
 ('data', 'NNS'),
 ('.', '.')]
    </div>
</details>

---

## 2. 명사와 형용사만 표시

```python
sent_nnjj = [word for word, pos in nltk.pos_tag(word_tok) if pos=='NN' or pos=='JJ']
print(sent_nnjj)
```

<details>
    <summary>output</summary>
    <div markdown='1'>
        ['Natural',
 'language',
 'processing',
 'subfield',
 'computer',
 'science',
 'information',
 'engineering',
 'artificial',
 'intelligence',
 'human',
 'natural',
 'particular',
 'program',
 'large',
 'natural',
 'language']
    </div>
</details>

---

## 3. bigram

```python
>>> bigram = [(a, b) for a, b in nltk.bigrams(sent_nnjj)]
>>> print(bigram)
[('Natural', 'language'),
 ('language', 'processing'),
 ('processing', 'subfield'),
...
 ('large', 'natural'),
 ('natural', 'language')]
```

<details>
    <summary>output</summary>
    <div markdown='1'>
        [('Natural', 'language'),
 ('language', 'processing'),
 ('processing', 'subfield'),
 ('subfield', 'computer'),
 ('computer', 'science'),
 ('science', 'information'),
 ('information', 'engineering'),
 ('engineering', 'artificial'),
 ('artificial', 'intelligence'),
 ('intelligence', 'human'),
 ('human', 'natural'),
 ('natural', 'particular'),
 ('particular', 'program'),
 ('program', 'large'),
 ('large', 'natural'),
 ('natural', 'language')]
    </div>
</details>

---

## 4. trigram

```python
>>> trigram = [(a, b, c) for a, b, c in nltk.trigrams(sent_nnjj)]
>>> print(trigram)
[('Natural', 'language', 'processing'),
 ('language', 'processing', 'subfield'),
 ('processing', 'subfield', 'computer'),
...
 ('particular', 'program', 'large'),
 ('program', 'large', 'natural'),
 ('large', 'natural', 'language')]
```

<details>
    <summary>output</summary>
    <div markdown='1'>
        [('Natural', 'language', 'processing'),
 ('language', 'processing', 'subfield'),
 ('processing', 'subfield', 'computer'),
 ('subfield', 'computer', 'science'),
 ('computer', 'science', 'information'),
 ('science', 'information', 'engineering'),
 ('information', 'engineering', 'artificial'),
 ('engineering', 'artificial', 'intelligence'),
 ('artificial', 'intelligence', 'human'),
 ('intelligence', 'human', 'natural'),
 ('human', 'natural', 'particular'),
 ('natural', 'particular', 'program'),
 ('particular', 'program', 'large'),
 ('program', 'large', 'natural'),
 ('large', 'natural', 'language')]
    </div>
</details>

---

## 5. n-gram

```python
>>> ngram = [(a, b, c, d) for a, b, c, d in nltk.ngrams(sent_nnjj, 4)]
>>> print(ngram)
[('Natural', 'language', 'processing', 'subfield'),
 ('language', 'processing', 'subfield', 'computer'),
...
 ('natural', 'particular', 'program', 'large'),
 ('particular', 'program', 'large', 'natural'),
 ('program', 'large', 'natural', 'language')]
```

<details>
    <summary>output</summary>
    <div markdown='1'>
        [('Natural', 'language', 'processing', 'subfield'),
 ('language', 'processing', 'subfield', 'computer'),
 ('processing', 'subfield', 'computer', 'science'),
 ('subfield', 'computer', 'science', 'information'),
 ('computer', 'science', 'information', 'engineering'),
 ('science', 'information', 'engineering', 'artificial'),
 ('information', 'engineering', 'artificial', 'intelligence'),
 ('engineering', 'artificial', 'intelligence', 'human'),
 ('artificial', 'intelligence', 'human', 'natural'),
 ('intelligence', 'human', 'natural', 'particular'),
 ('human', 'natural', 'particular', 'program'),
 ('natural', 'particular', 'program', 'large'),
 ('particular', 'program', 'large', 'natural'),
 ('program', 'large', 'natural', 'language')]
    </div>
</details>





