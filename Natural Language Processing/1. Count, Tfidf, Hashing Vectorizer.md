:baby_chick: 교재: 텐서플로2와 러닝머신으로 시작하는 자연어 처리

**02 자연어 처리 개발 준비**

__사이킷런을 이용한 특징 추출__

1. ***CountVectorizer***: 사전 기반/ 각 텍스트에서 횟수를 기준으로 특징 추출

2. ***TfidfVectorizer***: 사전 기반/ `TF-IDF` 값으로 특징 추출

- 사전 기반(corpus)의 문제점
  - 사전이 크다(메모리를 많이 차지하고 속도가 떨어진다)
  - 사전에 없는 단어 처리가 힘들다

3. ***HashingVectorizer***: 해시 함수 이용(속도 빠름)

:arrow_right:1, 2, 3의 공통점 : 텍스트를 벡터로 만든다(수치화 한다)

___



# 1. CountVectorizer

- 텍스트 데이터에서 횟수를 기준으로 특징 추출
- 어떤 단위(단어, 형태소 등)로 횟수를 셀 것인지 선택
- 주로 단어를 기준으로 횟수 측정

### 코드

1. 객체 생성 및 사전(vocabulary)구축
2. 각 단어를 사전의 index로 표현
3. 횟수를 기준으로 텍스트를 벡터화(수치화 작업)

```python
from sklearn.feature_extraction.text import CountVectorizer

text_data = ['나는 배가 고프다',
            '내일 점심 뭐먹지',
            '내일 공부 해야겠다',
            '점심 먹고 공부해야지']
count_vectorizer = CountVectorizer()
count_vectorizer.fit(text_data)
print(count_vectorizer.vocabulary_)
```

```python
# output
{'나는': 2, '배가': 6, '고프다': 0, '내일': 3, '점심': 7, '뭐먹지': 5, '공부': 1, '해야겠다': 8, '먹고': 4, '해야지': 9}
```

```python
>>> sentence = [text_data[0]]
>>> print(count_vectorizer.transform(sentence))
  (0, 0)	1
  (0, 2)	1
  (0, 6)	1

>>> print(count_vectorizer.transform(sentence).toarray())
[[1 0 1 0 0 0 1 0 0 0]]

>>> print(count_vectorizer.transform(text_data).toarray())
[[1 0 1 0 0 0 1 0 0 0]
 [0 0 0 1 0 1 0 1 0 0]
 [0 1 0 1 0 0 0 0 1 0]
 [0 1 0 0 1 0 0 1 0 1]]

# 단어 순서를 고려하지 않는다는 단점이 있다.
```

```python
# 구조 알아보기
>>> vec_1 = count_vectorizer.transform(sentence)
<1x10 sparse matrix of type '<class 'numpy.int64'>'
	with 3 stored elements in Compressed Sparse Row format>
```

- 데이터 대부분이 0인 구조를 sparse 하다고 한다.
- 크기가 큰 사전을 위해 CSR구조가 필요하다.



### vocabulary 만들기

```python
>>> word2idx = count_vectorizer.vocabulary_
>>> idx2word = {v:k for (k, v) in word2idx.items()}
>>> print(word2idx)
{'고프다': 0,
 '공부': 1,
 '나는': 2,
 '내일': 3,
 '먹고': 4,
 '뭐먹지': 5,
 '배가': 6,
 '점심': 7,
 '해야겠다': 8,
 '해야지': 9}

>>> word2idx['공부']
1
```

```python
>>> print(idx2word)
{0: '고프다',
 1: '공부',
 2: '나는',
 3: '내일',
 4: '먹고',
 5: '뭐먹지',
 6: '배가',
 7: '점심',
 8: '해야겠다',
 9: '해야지'}

>>> idx2word[7]
'점심'
```

---

# 2. TfidfVectorizer

- TF(Term Frequency): 특정 단어가 하나의 데이터 안에서 등장하는 횟수
- DF(Document Frequency): 문서 빈도 값. 특정 단어가 여러 데이터에 자주 등장하는지를 알려주는 지표
- IDF(Inverse Document Frequency): DF의 역수
- TF-IDF는 TF, IDF를 곱해서 사용

### 코드

```python
from sklearn.feature_extraction.text import TfidfVectorizer
text_data = ['나는 배가 고프다',
            '내일 점심 뭐먹지',
            '내일 공부 해야겠다',
            '점심 먹고 공부해야지']

tfidf_vectorizer = TfidfVectorizer
tfidf_vectorizer.fit(text_data)
print(tfidf_vectorizer.vocabulary_)
```

```python
# output
{'나는': 2, '배가': 6, '고프다': 0, '내일': 3, '점심': 7, '뭐먹지': 5, '공부': 1, '해야겠다': 8, '먹고': 4, '해야지': 9}
```

```python
>>> sentence = [text_data[0]]
>>> print(tfidf_vectorizer.transform(sentence).toarray())
[[0.57735027 0.         0.57735027 0.         0.         0.
  0.57735027 0.         0.         0.        ]]
```

___

# 3. HashingVectorizer

```python
from sklearn.feature_extraction.text import HashingVectorizer
text_data = ['나는 배가 고프다',
            '내일 점심 뭐먹지',
            '내일 공부 해야겠다',
            '점심 먹고 공부해야지']

hash_vectorizer = HashingVectorizer(n_features=10)
hash_vectorizer.fit(text_data)

sentenct = [text_data[0]]
print(hash_vectorizer.transform(sentence).toarray())
```

```python
# output
[[ 0.          0.57735027  0.          0.          0.57735027  0.
  -0.57735027  0.          0.          0.        ]]
```

```python
>>> hash('나는') % 100 # 무조건 100사이의 숫자로 나옴
72

>>> hash('sjdkfjkslfj') % 100 # 모르는 단어도 수치화할 수 있음
18
```

